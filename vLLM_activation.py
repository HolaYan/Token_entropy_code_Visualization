import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen2_5_VLForConditionalGeneration, AutoProcessor
# from qwen_vl_utils import process_vision_info  # ðŸ”¥ æ³¨é‡ŠæŽ‰è¿™è¡Œï¼Œä¸ä½¿ç”¨process_vision_info
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.patches import Rectangle
from collections import defaultdict
import re
import os
from matplotlib.colors import LinearSegmentedColormap
from PIL import Image

# Set cache directories
os.environ["TRANSFORMERS_CACHE"] = "/gpfs/scratch/lh3862/project/VLLMreasoning/Model"
os.environ["HF_HOME"] = "/gpfs/scratch/lh3862/project/VLLMreasoning/Model"

# ðŸ”¥ æŒ‡å®šä½¿ç”¨GPU 1ï¼ˆä»Ž0å¼€å§‹æ•°çš„ç¬¬1å·GPUï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# ðŸ”¥ æˆ–è€…ä½ ä¹Ÿå¯ä»¥åœ¨è¿è¡Œè„šæœ¬æ—¶æŒ‡å®šï¼š
# CUDA_VISIBLE_DEVICES=1 python your_script.py

# ðŸ”¥ æ¿€æ´»æ”¶é›†å™¨ - ä¸“é—¨æ”¶é›†æœ€åŽä¸€å±‚MLP
def setup_last_layer_mlp_hook(model):
    """è®¾ç½®æœ€åŽä¸€å±‚MLPçš„activation hook"""
    activities = []
    hooks = []
    
    def last_mlp_hook_fn(module, input, output):
        # ðŸ”¥ åªä¿å­˜æœ€åŽä¸€ä¸ªtokenï¼Œç«‹å³è½¬CPUä»¥èŠ‚çœå†…å­˜
        last_token_act = output[:, -1, :].detach().cpu()
        activities.append(last_token_act)
    
    # æ‰¾åˆ°æœ€åŽä¸€å±‚çš„MLP - æ ¹æ®æ¨¡åž‹ç»“æž„åº”è¯¥æ˜¯ model.layers.63.mlp
    last_layer_mlp = None
    for name, module in model.named_modules():
        print(name)
        if name == "model.layers.63.mlp":  # æœ€åŽä¸€å±‚ï¼ˆç¬¬63å±‚ï¼‰çš„MLP
            last_layer_mlp = module
            break
    # import pdb;pdb.set_trace()
    if last_layer_mlp is not None:
        hook = last_layer_mlp.register_forward_hook(last_mlp_hook_fn)
        hooks.append(hook)
        print(f"âœ… æˆåŠŸä¸ºæœ€åŽä¸€å±‚MLP (model.layers.63.mlp) è®¾ç½®hook")
    else:
        print("âŒ æœªæ‰¾åˆ°æœ€åŽä¸€å±‚MLP")
    
    return activities, hooks

def clean_hooks(hooks):
    """æ¸…ç†hooks"""
    for h in hooks:
        h.remove()

def analyze_vl_model_with_activations(model, processor, text, image_path=None, max_new_tokens=10, collect_activations=False):
    """åˆ†æžè§†è§‰è¯­è¨€æ¨¡åž‹çš„ç”Ÿæˆ + activationæ”¶é›†"""
    
    # ðŸ”¥ è®¾ç½®activationæ”¶é›†
    activities, hooks = None, []
    if collect_activations:
        activities, hooks = setup_last_layer_mlp_hook(model)
        activities.clear()
    
    # ðŸ”¥ ä½¿ç”¨ä¸Žå·¥ä½œä»£ç ç›¸åŒçš„è¾“å…¥æ ¼å¼
    if image_path and os.path.exists(image_path):
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # ä½¿ç”¨å·¥ä½œä»£ç çš„messagesæ ¼å¼
        messages = [
            {"role": "user", "content": [
                {"type": "text", "text": text},
                {"type": "image_url", "image_url": {"url": f"file://{image_path}"}},
            ]},
        ]
    else:
        image = None
        # çº¯æ–‡æœ¬è¾“å…¥
        messages = [
            {"role": "user", "content": [
                {"type": "text", "text": text}
            ]}
        ]
    
    # ðŸ”¥ ä½¿ç”¨ä¸Žå·¥ä½œä»£ç ç›¸åŒçš„å¤„ç†æ–¹å¼
    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    if image is not None:
        inputs = processor(text=text_input, images=image, return_tensors="pt")
    else:
        inputs = processor(text=text_input, return_tensors="pt")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡ - ç¡®ä¿ä½¿ç”¨GPU 1
    device = torch.device("cuda:0")  # è¿™é‡Œæ˜¯0å› ä¸ºCUDA_VISIBLE_DEVICESå·²ç»è®¾ç½®ä¸º1
    inputs = inputs.to(device)
    
    print(f"è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
    
    # ðŸ”¥ ä½¿ç”¨ä¸Žå·¥ä½œä»£ç ç›¸åŒçš„ç”Ÿæˆæ–¹å¼
    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)
    
    # ðŸ”¥ ä½¿ç”¨ä¸Žå·¥ä½œä»£ç ç›¸åŒçš„è¾“å‡ºå¤„ç†æ–¹å¼
    generated_ids = output_ids[0][inputs.input_ids.shape[1]:]
    response = processor.decode(generated_ids, skip_special_tokens=True)
    
    print(f"ç”Ÿæˆçš„å“åº”: {response}")
    
    # å°†token IDè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    tokens = []
    for token_id in generated_ids:
        token_str = processor.tokenizer.decode(token_id, skip_special_tokens=True)
        tokens.append(token_str)
    
    # ðŸ”¥ å¤„ç†activations
    activations = None
    if collect_activations and activities:
        print(f"æ”¶é›†åˆ° {len(activities)} ä¸ªforward passçš„activations")
        # ç”±äºŽç”Ÿæˆè¿‡ç¨‹ä¸­æ¯ä¸ªtokenéƒ½ä¼šè§¦å‘ä¸€æ¬¡forward passï¼Œ
        # æˆ‘ä»¬éœ€è¦æå–å¯¹åº”ç”Ÿæˆtokençš„activations
        if len(activities) > 0:
            # å–æœ€åŽå‡ ä¸ªforward passçš„activationsï¼ˆå¯¹åº”ç”Ÿæˆçš„tokensï¼‰
            num_gen_tokens = len(generated_ids)
            relevant_activations = activities[-num_gen_tokens:] if len(activities) >= num_gen_tokens else activities
            
            if relevant_activations:
                # åˆå¹¶æ‰€æœ‰ç”Ÿæˆtokençš„activations
                activations = torch.cat(relevant_activations, dim=0)  # shape: [num_generated_tokens, hidden_dim]
                print(f"æœ€ç»ˆactivationså½¢çŠ¶: {activations.shape}")
    
    # æ¸…ç†hooks
    if hooks:
        clean_hooks(hooks)
    
    return tokens, activations, response

def analyze_activations(activations):
    """åˆ†æžactivationæ•°æ®"""
    if activations is None:
        print("æ²¡æœ‰æ”¶é›†åˆ°activationæ•°æ®")
        return
    
    print(f"Activationså½¢çŠ¶: {activations.shape}")
    print(f"Mean activation: {activations.mean():.4f}")
    print(f"Std activation: {activations.std():.4f}")
    print(f"Min activation: {activations.min():.4f}")
    print(f"Max activation: {activations.max():.4f}")
    
    # å¯é€‰ï¼šåˆ†æžæ¯ä¸ªtokençš„activationç»Ÿè®¡
    print("\næ¯ä¸ªç”Ÿæˆtokençš„activationç»Ÿè®¡:")
    for i in range(activations.shape[0]):
        token_act = activations[i]
        print(f"Token {i}: mean={token_act.mean():.4f}, std={token_act.std():.4f}")

def visualize_activations(activations, tokens, save_path="VL_activation_heatmap.png"):
    """å¯è§†åŒ–activation"""
    if activations is None:
        print("æ²¡æœ‰activationæ•°æ®å¯è§†åŒ–")
        return
    
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    act_np = activations.float().numpy()
    
    # å¦‚æžœç»´åº¦å¤ªå¤§ï¼Œåªæ˜¾ç¤ºå‰100ä¸ªç»´åº¦
    if act_np.shape[1] > 100:
        act_np = act_np[:, :100]
    
    plt.imshow(act_np.T, cmap='viridis', aspect='auto')
    plt.colorbar(label='Activation Value')
    plt.xlabel('Generated Token Position')
    plt.ylabel('Hidden Dimension')
    plt.title('Last Layer MLP Activations for Generated Tokens')
    
    # æ·»åŠ tokenæ ‡ç­¾
    if len(tokens) <= 20:  # åªæœ‰tokenæ•°é‡ä¸å¤ªå¤šæ—¶æ‰æ˜¾ç¤º
        plt.xticks(range(len(tokens)), [f"'{t}'" for t in tokens], rotation=45)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ðŸ”¥ æ£€æŸ¥GPUçŠ¶æ€
    print("GPUçŠ¶æ€æ£€æŸ¥:")
    if torch.cuda.is_available():
        print(f"å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  å†…å­˜ä½¿ç”¨: {memory_allocated:.2f}GB / {memory_total:.2f}GB")
        print(f"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
    else:
        print("CUDAä¸å¯ç”¨")
    
    # åŠ è½½æ¨¡åž‹
    model_path = "Qwen/Qwen2.5-VL-32B-Instruct"
    dir_path = "/gpfs/scratch/lh3862/project/VLLMreasoning/Model"
    
    print("ðŸ”¥ æ­£åœ¨åŠ è½½æ¨¡åž‹...")
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        attn_implementation="eager",
        device_map={"": 0},  # è¿™é‡Œçš„0å®žé™…ä¸Šæ˜¯GPU 1ï¼ˆå› ä¸ºCUDA_VISIBLE_DEVICES="1"ï¼‰
        cache_dir=dir_path,
    )
    
    processor = AutoProcessor.from_pretrained(
        model_path,
        cache_dir=dir_path
    )
    
    print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆ")
    
    # æµ‹è¯•è¾“å…¥
    text = "Please reconstruct following images:"
    image_path = '/gpfs/scratch/lh3862/project/speedread_video/inference_codes/MMLU_5shot.png'
    
    print(f"ðŸ”¥ å¼€å§‹åˆ†æžæ¨¡åž‹...")
    print(f"æ–‡æœ¬è¾“å…¥: {text}")
    print(f"å›¾åƒè·¯å¾„: {image_path}")
    
    print("\n=== æµ‹è¯•2: æ”¶é›†activation ===")
    tokens2, activations2, response2 = analyze_vl_model_with_activations(
        model, processor, text, image_path=image_path, 
        max_new_tokens=1024, collect_activations=True
    )
    
    # åˆ†æžactivation
    if activations2 is not None:
        analyze_activations(activations2)
        
        # ä¿å­˜activations
        torch.save(activations2, 'last_layer_mlp_activations.pt')
        print("âœ… Activationså·²ä¿å­˜åˆ° last_layer_mlp_activations.pt")
        
        # å¯è§†åŒ–activations
        visualize_activations(activations2, tokens2)
    else:
        print("âŒ æœªæ”¶é›†åˆ°activationæ•°æ®")